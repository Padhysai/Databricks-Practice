{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "235c2c93-d8dd-46e2-85bc-4e1e9518623e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql.functions import current_timestamp, row_number, col\n",
    "from pyspark.sql import Window\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class DatabaseInitializer:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "\n",
    "    def init_databases(self):\n",
    "        try:\n",
    "            self.spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "            self.spark.sql(\"CREATE DATABASE IF NOT EXISTS silver\")\n",
    "            self.spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "            logging.info(\"Databases initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing databases: {e}\")\n",
    "\n",
    "    def get_config(self):\n",
    "        return [\n",
    "        {\n",
    "            \"file_path\": \"dbfs:/FileStore/shared_uploads/saiprasadpadhy@gmail.com/media_customer_reviews\",\n",
    "            \"tbl_name\": \"media_customer_reviews\",\n",
    "            \"primary_keys\": [\"new_id\"],\n",
    "            \"type\": \"dim\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"dbfs:/FileStore/shared_uploads/saiprasadpadhy@gmail.com/media_gold_reviews_chunked\",\n",
    "            \"tbl_name\": \"media_gold_reviews_chunked\",\n",
    "            \"primary_keys\": [\"franchiseID\", \"chunk_id\"],\n",
    "            \"type\": \"fact\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"dbfs:/FileStore/shared_uploads/saiprasadpadhy@gmail.com/sales_customers\",\n",
    "            \"tbl_name\": \"sales_customers\",\n",
    "            \"primary_keys\": [\"customerID\"],\n",
    "            \"type\": \"dim\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"dbfs:/FileStore/shared_uploads/saiprasadpadhy@gmail.com/sales_franchises\",\n",
    "            \"tbl_name\": \"sales_franchises\",\n",
    "            \"primary_keys\": [\"franchiseID\"],\n",
    "            \"type\": \"dim\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"dbfs:/FileStore/shared_uploads/saiprasadpadhy@gmail.com/sales_suppliers\",\n",
    "            \"tbl_name\": \"sales_suppliers\",\n",
    "            \"primary_keys\": [\"supplierID\"],\n",
    "            \"type\": \"dim\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"dbfs:/FileStore/shared_uploads/saiprasadpadhy@gmail.com/sales_transactions\",\n",
    "            \"tbl_name\": \"sales_transactions\",\n",
    "            \"primary_keys\": [\"transactionID\"],\n",
    "            \"type\": \"fact\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "class BronzeLayerLoader:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "\n",
    "    def load(self, db: str, parquet_path: str, table_name: str):\n",
    "        try:\n",
    "            table_exists = self.spark.catalog.tableExists(f\"{db}.{table_name}\")\n",
    "            df = self.spark.read.parquet(parquet_path).withColumn('inserted_at', current_timestamp())\n",
    "            mode = \"append\" if table_exists else \"overwrite\"\n",
    "            df.write.format(\"delta\").mode(mode).saveAsTable(f\"{db}.{table_name}\")\n",
    "            logging.info(f\"Loaded data into bronze table: {db}.{table_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load data into bronze table {db}.{table_name}: {e}\")\n",
    "    \n",
    "    def load_with_autoloader(self, db: str, parquet_path: str, table_name: str):\n",
    "        try:\n",
    "            table_exists = self.spark.catalog.tableExists(f\"{db}.{table_name}\")\n",
    "            df = self.spark.readStream \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"cloudFiles.format\", \"parquet\") \\\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"/tmp/schema/{db}/{table_name}\")\\\n",
    "                .load(parquet_path) \\\n",
    "                .withColumn(\"inserted_at\", current_timestamp())\n",
    "\n",
    "            checkpoint_path = f\"/tmp/checkpoints/{db}_{table_name}\"\n",
    "\n",
    "            query = df.writeStream \\\n",
    "                .format(\"delta\") \\\n",
    "                .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .trigger(once=True) \\\n",
    "                .toTable(f\"{db}.{table_name}\")\n",
    "\n",
    "            query.awaitTermination()\n",
    "            logging.info(f\"Auto Loader loaded data into bronze table: {db}.{table_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Auto Loader failed for bronze table {db}.{table_name}: {e}\")\n",
    "\n",
    "\n",
    "    def cleanup(self, table_name: str):\n",
    "        table_path = f\"/user/hive/warehouse/{table_name}\"\n",
    "        try:\n",
    "            dbutils.fs.rm(f\"dbfs:{table_path}\", True)\n",
    "            logging.info(f\"Removed files for table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Files not found or failed to delete for {table_name}: {e}\")\n",
    "        try:\n",
    "            self.spark.sql(f\"DROP TABLE {table_name}\")\n",
    "            logging.info(f\"Dropped table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Table not found or failed to drop: {table_name}: {e}\")\n",
    "\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "\n",
    "    def scd1_merge(self, src_db: str, dest_db: str, tbl_name: str, primary_keys: list):\n",
    "        try:   \n",
    "            window_spec = Window.partitionBy(primary_keys).orderBy(col(\"inserted_at\").desc())\n",
    "            df_bronze = (\n",
    "                self.spark.table(f\"{src_db}.{tbl_name}\")\n",
    "                .withColumn(\"row_num\", row_number().over(window_spec))\n",
    "                .filter(\"row_num = 1\")\n",
    "                .drop(\"row_num\")\n",
    "                .withColumn(\"updated_at\", current_timestamp())\n",
    "                .drop(\"inserted_at\")\n",
    "            )\n",
    "\n",
    "            df_bronze.createOrReplaceTempView(\"bronze_tmp\")\n",
    "            table_exists = self.spark.catalog.tableExists(f\"{dest_db}.{tbl_name}\")\n",
    "\n",
    "            if not table_exists:\n",
    "                df_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{dest_db}.{tbl_name}\")\n",
    "                logging.info(f\"Created new silver table: {tbl_name}\")\n",
    "                return\n",
    "\n",
    "            pk_conditions = \" AND \".join([f\"target.{col} = source.{col}\" for col in primary_keys])\n",
    "            merge_sql = f\"\"\"\n",
    "                MERGE INTO {dest_db}.{tbl_name} AS target\n",
    "                USING bronze_tmp AS source\n",
    "                ON {pk_conditions}\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\"\n",
    "            self.spark.sql(merge_sql)\n",
    "            logging.info(f\"SCD Type 1 merge completed: {src_db}.{tbl_name} -> {dest_db}.{tbl_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during SCD1 merge for {tbl_name}: {e}\")\n",
    "\n",
    "    def load_to_gold(self, src_db: str, dest_db: str, tbl_name: str, tbl_type: str):\n",
    "        try:\n",
    "            df = self.spark.table(f\"{src_db}.{tbl_name}\")\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{dest_db}.{tbl_type}_{tbl_name}\")\n",
    "            logging.info(f\"Loaded data into gold table: {dest_db}.{tbl_type}_{tbl_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load gold table {tbl_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7044da4e-3642-465f-8b16-bdcaad6bb12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:07:51,211 - INFO - Received command c on object id p0\n2025-05-20 16:07:51,285 - INFO - Databases initialized successfully.\n2025-05-20 16:07:51,450 - ERROR - Auto Loader failed for bronze table bronze.media_customer_reviews: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.\n2025-05-20 16:07:59,099 - INFO - SCD Type 1 merge completed: bronze.media_customer_reviews -> silver.media_customer_reviews\n2025-05-20 16:08:03,790 - INFO - Loaded data into gold table: gold.dim_media_customer_reviews\n2025-05-20 16:08:04,028 - ERROR - Auto Loader failed for bronze table bronze.media_gold_reviews_chunked: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.\n2025-05-20 16:08:11,516 - INFO - SCD Type 1 merge completed: bronze.media_gold_reviews_chunked -> silver.media_gold_reviews_chunked\n2025-05-20 16:08:16,915 - INFO - Loaded data into gold table: gold.fact_media_gold_reviews_chunked\n2025-05-20 16:08:17,125 - ERROR - Auto Loader failed for bronze table bronze.sales_customers: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.\n2025-05-20 16:08:23,628 - INFO - SCD Type 1 merge completed: bronze.sales_customers -> silver.sales_customers\n2025-05-20 16:08:27,954 - INFO - Loaded data into gold table: gold.dim_sales_customers\n2025-05-20 16:08:28,152 - ERROR - Auto Loader failed for bronze table bronze.sales_franchises: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.\n2025-05-20 16:08:35,544 - INFO - SCD Type 1 merge completed: bronze.sales_franchises -> silver.sales_franchises\n2025-05-20 16:08:40,270 - INFO - Loaded data into gold table: gold.dim_sales_franchises\n2025-05-20 16:08:40,448 - ERROR - Auto Loader failed for bronze table bronze.sales_suppliers: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.\n2025-05-20 16:08:46,624 - INFO - SCD Type 1 merge completed: bronze.sales_suppliers -> silver.sales_suppliers\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "initializer = DatabaseInitializer(spark)\n",
    "initializer.init_databases()\n",
    "config = initializer.get_config()\n",
    "\n",
    "bronze_loader = BronzeLayerLoader(spark)\n",
    "transformer = DataTransformer(spark)\n",
    "\n",
    "for entry in config:\n",
    "    bronze_loader.load_with_autoloader(\"bronze\", entry[\"file_path\"], entry[\"tbl_name\"])\n",
    "    transformer.scd1_merge(\"bronze\", \"silver\", entry[\"tbl_name\"], entry[\"primary_keys\"])\n",
    "    transformer.load_to_gold(\"silver\", \"gold\", entry[\"tbl_name\"], entry[\"type\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00aa70d9-b29d-4c39-8bfa-6f5bc7767fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Most sold products to identify the top-selling items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef52fba-0c47-4680-82a9-de7587aafd47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 10:35:17,062 - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n|             product|no_of_products_sold|\n+--------------------+-------------------+\n|  Golden Gate Ginger|               3865|\n|     Outback Oatmeal|               3733|\n|Austin Almond Bis...|               3716|\n|       Tokyo Tidbits|               3662|\n|         Pearly Pies|               3595|\n|       Orchard Oasis|               3586|\n+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"gold.fact_sales_transactions\")\n",
    "result_df = df.groupBy(\"product\") \\\n",
    "              .sum(\"quantity\") \\\n",
    "              .withColumnRenamed(\"sum(quantity)\", \"no_of_products_sold\") \\\n",
    "              .orderBy(\"no_of_products_sold\", ascending=False)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c845bc-1592-47f3-b455-ab3149038246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Suppliers provide ingredients to the most franchises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d47e25-231c-45da-accf-01f859564481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n|supplierID|no_of_franchises|\n+----------+----------------+\n|   4000022|               1|\n|   4000034|               1|\n|   4000021|               1|\n|   4000005|               1|\n|   4000003|               1|\n|   4000044|               1|\n|   4000004|               1|\n|   4000037|               1|\n|   4000039|               1|\n|   4000047|               1|\n|   4000045|               1|\n|   4000031|               1|\n|   4000009|               1|\n|   4000015|               1|\n|   4000019|               1|\n|   4000013|               1|\n|   4000026|               1|\n|   4000018|               1|\n|   4000028|               1|\n|   4000032|               1|\n+----------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"gold.dim_sales_franchises\")\n",
    "result_df = df.groupBy(\"supplierID\") \\\n",
    "              .count() \\\n",
    "              .withColumnRenamed(\"count\", \"no_of_franchises\") \\\n",
    "              .orderBy(\"no_of_franchises\", ascending=False)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "811e02d9-6209-4c1d-ae09-42da277abfe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Total sales per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6511cf9-6be8-4fbc-b336-d765bd6b912f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 10:36:47,478 - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n|sales_month|sales_amount|\n+-----------+------------+\n|          5|     66471.0|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, sum, col, to_date, expr\n",
    "\n",
    "df = spark.table(\"gold.fact_sales_transactions\")\n",
    "\n",
    "df = df.withColumn(\"totalPrice\", col(\"totalPrice\").cast(\"double\"))\n",
    "df = df.withColumn(\"sales_month\", expr(\"extract(month from dateTime)\"))\n",
    "\n",
    "result_df = df.groupBy(\"sales_month\") \\\n",
    "              .agg(sum(\"totalPrice\").alias(\"sales_amount\")) \\\n",
    "              .orderBy(\"sales_amount\", ascending=False)\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4263325916666923,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bakehouse Assignment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}